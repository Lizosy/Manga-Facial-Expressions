# -*- coding: utf-8 -*-
"""Transfer Learning for Manga Facial Expressions 1000

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1prBgdyOF8Ha3NGyPZFgZXUQfgR3_I_Qu
"""

import tensorflow as tf 
import numpy as np
import os
import matplotlib.pyplot as plt 
tf.__version__

Name0=os.listdir('./manga-faces-dataset')
Name=sorted(Name0)
n=len(Name)
N=list(range(n))
normal_mapping=dict(zip(Name,N)) 
reverse_mapping=dict(zip(N,Name))

"""**Part 1 : Preprocessing with ImageDataGenerator Preprocessing โดยใช้ ImageDataGenerator**
 
โดยปกติที่เราต้องทำ Image Classification เราต้องทำหลายขั้นตอนมาก ไม่ว่าจะเป็น Re-Scale( แปลงRGBจาก 0–255 เป็น 0–1 ) , Train-Valid-Test Split , Data Augment , หรือจะเป็นแปลง Image to Numpy ซึ่งขั้นตอนเหล่าจะหมดไป เมื่อเราใช้ ‘Image Data Generator’

Prepare ImageDataGenerator
- https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator
"""

img_generator = tf.keras.preprocessing.image.ImageDataGenerator(
                            #rotation_range=90,
                            brightness_range=(0.5,1), 
                            #shear_range=0.2, 
                            #zoom_range=0.2,
                            channel_shift_range=0.2,
                            horizontal_flip=False,
                            vertical_flip=False,
                            rescale=1./255,
                            validation_split=0.3)

"""Prepare img_generator_flow
- โดยเราจะแบ่งข้อมูลเป็นทั้งสองอย่างนั้นคือ
 Validation set ชุดข้อมูลสำหรับเลือโมเดลที่ดีที่สุดโดยเราจะเลือกโมเดลที่ดีที่สุดในการเทรนข้อมูลครบ1รอบ(1 epoch)
 Test set ชุดข้อมูลที่ใช้ทดสอบว่าโมเดลทำงานได้ดีแค่ไหนเมื่อเจอกับชุดข้อมูลที่ไม่เคยเห็นมาก่อน
"""

root_dir = './manga-faces-dataset'

img_generator_flow_train = img_generator.flow_from_directory(
    directory=root_dir,
    target_size=(224, 224),
    batch_size=32,
    shuffle=True,
    subset="training")

img_generator_flow_valid = img_generator.flow_from_directory(
    directory=root_dir,
    target_size=(224, 224),
    batch_size=32,
    shuffle=True,
    subset="validation")

"""Visualize a batch of images

> ภาพที่มาจาก train set


"""

imgs, labels = next(iter(img_generator_flow_train))
for img, label in zip(imgs, labels):
    value=np.argmax(label)
    plt.imshow(img)
    plt.title(reverse_mapping[value])
    plt.show()

"""# Part 2 Transfer Learning
Import a pretrained model
 
 โดย model ที่ผมจะนำมาใช้ในวันนี้คือ InceptionV3 
# - https://www.tensorflow.org/api_docs/python/tf/keras/applications/InceptionV3
"""

base_model = tf.keras.applications.InceptionV3(input_shape=(224,224,3),
                                               include_top=False,
                                               weights = "imagenet"
                                               )

"""Set the weights of the imported model"""

base_model.trainable = False

tf.keras.applications.inception_v3.InceptionV3(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)

"""สร้างโมเดลที่สมบูรณ์ของเราโดยเพิ่มเลเยอร์สุดท้ายที่ปรับให้เข้ากับข้อมูลของเรา"""

model = tf.keras.Sequential([
    base_model,
    tf.keras.layers.MaxPooling2D(),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(11, activation="softmax")
])

model.summary()

"""Compile model"""

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = 0.0001),
              loss = tf.keras.losses.CategoricalCrossentropy(),
              metrics = [tf.keras.metrics.CategoricalAccuracy()])

"""Train the model"""

model.fit(img_generator_flow_train, 
          validation_data=img_generator_flow_valid, 
          steps_per_epoch=10, epochs=100)

"""# Visualize accuracy and loss
ตรวจสอบความแม่นยำของ model 
"""

# Visualise train / Valid Accuracy
plt.plot(model.history.history["categorical_accuracy"], c="r", label="train_accuracy")
plt.plot(model.history.history["val_categorical_accuracy"], c="b", label="test_accuracy")
plt.legend(loc="upper left")
plt.show()

""" ตรวจสอบค่า loss"""

# Visualise train / Valid Loss
plt.plot(model.history.history["loss"], c="r", label="train_loss")
plt.plot(model.history.history["val_loss"], c="b", label="test_loss")
plt.legend(loc="upper left")
plt.show()

"""# Part 3 : Interpretation with Grad Cam
- โดยในขั้นตอนสุดท้ายของการตรวจสอบ model ของผมจะใช้ Grad Cam เพื่อ Visualize สิ่งที่โมเดลเห็น และจะได้รู้ว่าที่โมเดลที่ทำนายออกมาเป็นผลลัพธ์ นั้นโมเดลพิจารณาจากจุดไหนของ Data หรือรูปภาพของเราเพื่อที่จะสามารถ
นำไปปรับปรุงข้อมูลในอนาคต โดยเจ้า Grad Cam จะทำงานร่วมกับ heatmap

Create imgs and labels
"""

imgs, labels = next(iter(img_generator_flow_valid))

for layer in model.layers:
    print(layer.name)

base_model = model.layers[0]

last_conv_layer_name = "mixed10"
classifier_layer_names = [layer.name for layer in model.layers][1:]

# We start by setting up the dependencies we will use

import numpy as np
import tensorflow as tf
from tensorflow import keras

# Display
from IPython.display import Image
import matplotlib.pyplot as plt
import matplotlib.cm as cm

"""**make_gradcam_heatmap**"""

# The Grad-CAM algorithm
def get_img_array(img_path, size):
    # `img` is a PIL image of size 299x299
    img = keras.preprocessing.image.load_img(img_path, target_size=size)
    # `array` is a float32 Numpy array of shape (299, 299, 3)
    array = keras.preprocessing.image.img_to_array(img)
    # We add a dimension to transform our array into a "batch"
    # of size (1, 299, 299, 3)
    array = np.expand_dims(array, axis=0)
    return array


def make_gradcam_heatmap(
    img_array, base_model, model, last_conv_layer_name, classifier_layer_names):
    # First, we create a model that maps the input image to the activations
    # of the last conv layer
    last_conv_layer = base_model.get_layer(last_conv_layer_name)
    last_conv_layer_model = keras.Model(base_model.inputs, last_conv_layer.output)

    # Second, we create a model that maps the activations of the last conv
    # layer to the final class predictions
    classifier_input = keras.Input(shape=last_conv_layer.output.shape[1:])
    x = classifier_input
    for layer_name in classifier_layer_names:
        x = model.get_layer(layer_name)(x)
    classifier_model = keras.Model(classifier_input, x)

    # Then, we compute the gradient of the top predicted class for our input image
    # with respect to the activations of the last conv layer
    with tf.GradientTape() as tape:
        # Compute activations of the last conv layer and make the tape watch it
        last_conv_layer_output = last_conv_layer_model(img_array)
        tape.watch(last_conv_layer_output)
        # Compute class predictions
        preds = classifier_model(last_conv_layer_output)
        top_pred_index = tf.argmax(preds[0])
        top_class_channel = preds[:, top_pred_index]

    # This is the gradient of the top predicted class with regard to
    # the output feature map of the last conv layer
    grads = tape.gradient(top_class_channel, last_conv_layer_output)

    # This is a vector where each entry is the mean intensity of the gradient
    # over a specific feature map channel
    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))

    # We multiply each channel in the feature map array
    # by "how important this channel is" with regard to the top predicted class
    last_conv_layer_output = last_conv_layer_output.numpy()[0]
    pooled_grads = pooled_grads.numpy()
    for i in range(pooled_grads.shape[-1]):
        last_conv_layer_output[:, :, i] *= pooled_grads[i]

    # The channel-wise mean of the resulting feature map
    # is our heatmap of class activation
    heatmap = np.mean(last_conv_layer_output, axis=-1)

    # For visualization purpose, we will also normalize the heatmap between 0 & 1
    heatmap = np.maximum(heatmap, 0) / np.max(heatmap)
    return heatmap

"""Predict"""

# Print what the top predicted class is
preds = model.predict(imgs)
pred_labels = tf.argmax(preds, axis = -1)

print("Prediction output:", preds)
print("Predicted label:", pred_labels)

"""**Create heatmap**"""

# Generate class activation heatmap
heatmaps = []

for img in imgs:
    heatmap = make_gradcam_heatmap(
    tf.expand_dims(img,axis=0),
        base_model, model, 
        last_conv_layer_name, 
        classifier_layer_names
  )
    heatmaps.append(heatmap)


# Display heatmap
plt.matshow(heatmaps[0])
plt.show()

from pathlib import Path

for img, pred_label, true_label, heatmap in zip(imgs, pred_labels, labels, heatmaps): 
    # We rescale heatmap to a range 0-255
    heatmap = np.uint8(255 * heatmap)

    # We use jet colormap to colorize heatmap
    jet = cm.get_cmap("jet")

    # We use RGB values of the colormap
    jet_colors = jet(np.arange(256))[:, :3]
    jet_heatmap = jet_colors[heatmap]

    # We create an image with RGB colorized heatmap
    jet_heatmap = keras.preprocessing.image.array_to_img(jet_heatmap)
    jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))
    jet_heatmap = keras.preprocessing.image.img_to_array(jet_heatmap)

    # Superimpose the heatmap on original image
    superimposed_img = jet_heatmap * 0.003 + img
    superimposed_img = keras.preprocessing.image.array_to_img(superimposed_img)

    # Save the superimposed image
    save_path = "saved_img.jpg"
    superimposed_img.save(save_path)

    # Display Grad CAM
    pred_file_path = np.argmax(img_generator_flow_valid.labels == pred_label)
    pred_label_name = Path(img_generator_flow_valid.filepaths[pred_file_path]).parent.name

    true_file_path = np.argmax(img_generator_flow_valid.labels == tf.argmax(true_label))
    true_label_name = Path(img_generator_flow_valid.filepaths[true_file_path]).parent.name

    print("Predicted label:",pred_label_name)
    print("True label:", true_label_name)

    display(Image(save_path))

"""สุดท้ายแล้วเรามาดูกันดีว่า model ของเรานั้น ทำงานได้ดีแค่ไหน"""

from sklearn.metrics import classification_report
print(classification_report(LABEL, PRED))

"""โดยสำหรับผมแล้วตัวของโมเดลถือว่าทำออกมาได้ดีเนื่องจาก ตัวของข้อมูล นั้นยังมีความจำเป็นที่จะต้องจำแนกให้มีให้ประสิทธิภาพ ให้มากยิ่งขึ้นเนื่อง
โดยอุปสรรคหลักของ ของการจำแนกนั้น มีทั้งการไม่ชัดเจนของหน้าตาตัวละคร
หรือแม้กกระทั้งลายเส้นที่แต่ต่างกันออกไปในแต่ละภาพ

"""